import streamlit as st
import time
import MeCab
from wordcloud import WordCloud
from collections import defaultdict
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import module


class Corpus3(module.Corpus):
    def __init__(self, session_state):
        super().__init__(session_state)
        self.fpath = './IPAfont00303/ipam.ttf'

    def morphological_analysis(self, text, selected_parts, stop_words):
        tagger = MeCab.Tagger()
        node = tagger.parseToNode(text)
        dic = defaultdict(int)
        morpheme_words = []

        while  node:
            surface = node.surface
            surface = self.preprocessing(surface)

            if node.feature.startswith('BOS/EOS') or surface in stop_words:
                pass
            else:
                f = node.feature.split(',')      
                pos1 = f[0]

                if pos1 in selected_parts and surface != '':
                    dic[(surface, pos1)] += 1
                    morpheme_words.append(surface)

            node = node.next
            
        sorted_items = sorted(dic.items(), key = lambda x: -x[1])
        merged_list = []

        for k, v in sorted_items:
            merged_list.append([k[0], k[1], v])

        self.full_text = ' '.join(morpheme_words)
        self.df = pd.DataFrame(merged_list)
        self.df.columns = ['è¡¨å±¤å½¢','å“è©','åº¦æ•°']
        self.keys = [_[0][0] for _ in sorted_items]
        self.values = [_[1] for _ in sorted_items[:10]]

    def unsupervised_learning(self):
        vectors = []
        self.keys_in_model = []

        for key in self.keys:
            if key in self.model.index_to_key:
                vectors.append(self.model[key])
                self.keys_in_model.append(key)
        # ä¸»æˆåˆ†åˆ†æ
        self.model2d = PCA(n_components=2, whiten=True)
        self.model2d.fit(np.array(vectors[:30]).T)

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        n_clusters = int(np.sqrt(len(self.keys_in_model))) + 1
        kmeans_model = KMeans(n_clusters=n_clusters, verbose=1, random_state=0) 
        kmeans_model.fit(vectors)

        self.cluster_labels = kmeans_model.labels_

    def get_wc_img(self):
        wc = WordCloud(background_color='white', width=960, height=720, font_path=self.fpath)
        wc.generate(self.full_text)
        wc.to_file('wc.png')    
        img = Image.open('wc.png')
        return img
    
    def draw_TOP10_chart(self):
        fig, ax = plt.subplots()
        ax.bar(self.keys[:10], self.values)
        fig.suptitle('å‡ºç¾é »åº¦ TOP10')
        st.pyplot(fig)

    def vector_plot(self):
        model2d = self.model2d
        fig, ax = plt.subplots()
        ax.scatter(model2d.components_[0],model2d.components_[1])

        for (x,y), name in zip(model2d.components_.T, self.keys_in_model[:30]):
            ax.annotate(name, (x,y))
        fig.suptitle('å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®åˆ†å¸ƒï¼ˆä¸Šä½ï¼“ï¼å˜èªã¾ã§ï¼‰')
        st.pyplot(fig)

    def get_cluster_to_words(self):
        cluster_to_words = defaultdict(list)
        for cluster_id, word in zip(self.cluster_labels, self.keys_in_model):
            cluster_to_words[cluster_id].append(word)
        return cluster_to_words


class Page3(module.Page):
    def __init__(self, session_state, cor):
        super().__init__(session_state, cor)
        self.all_parts = ['åè©','ä»£åè©','å½¢çŠ¶è©','é€£ä½“è©','å‰¯è©','æ¥ç¶šè©','æ„Ÿå‹•è©',
                        'å‹•è©','å½¢å®¹è©','åŠ©å‹•è©','åŠ©è©','æ¥é ­è¾','æ¥å°¾è¾','è¨˜å·','è£œåŠ©è¨˜å·']

    def get_user_df(self, input_users):
        df = self.cor.get_reports_df()[['writer', 'words']]

        if self.authority:
            if not input_users:
                user_df = df
            else:
                user_df = df.query('writer in @input_users')
        else:           
            user_df = df[df['writer']==self.user]
        
        return user_df

    def display_tabs(self):
        comment = st.empty()
        comment.text('Start!')

        st.subheader('è§£æçµæœ')

        with st.spinner('Wait for it...'):         
            tab1, tab2, tab3, tab4 = st.tabs(['ğŸŒ© Word cloud', 'ğŸ“Š TOP10', 'ğŸ“‰ Vectors', 'ğŸ’¥ Clusters'])
            
            with tab1:
                img = self.cor.get_wc_img()
                st.image(img, caption='ã‚ãªãŸã®æ—¥è¨˜ã®å†…å®¹ã‹ã‚‰ä½œæˆã•ã‚ŒãŸãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰', use_column_width=True)

            with tab2:
                self.cor.draw_TOP10_chart()

            st.sidebar.header('Below is a DataFrame:')
            st.sidebar.write('ğŸ—ƒ è©²å½“ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§', self.cor.df.head(1000))
            st.sidebar.caption('â€» åˆ—åã‚’ã‚¯ãƒªãƒƒã‚¯ã§ã‚½ãƒ¼ãƒˆå¯èƒ½')  
            
            comment.text('Done!')

        self.cor.unsupervised_learning() 

        with tab3:
            self.cor.vector_plot()
            st.caption('â€» å¯è¦–åŒ–ã®ãŸã‚ã€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã‚’300æ¬¡å…ƒã‹ã‚‰2æ¬¡å…ƒã«æ¬¡å…ƒå‰Šæ¸›ã—ã¦ã„ã¾ã™')  

        with tab4:
            st.write('ä¼¼ã¦ã„ã‚‹è¨€è‘‰ã®ã‚°ãƒ«ãƒ¼ãƒ—')
            cluster_to_words = self.cor.get_cluster_to_words()

            for i, words in enumerate(cluster_to_words.values()):
                cluster_df = pd.DataFrame(words).T
                cluster_df.index = [f'cluster {i}']
                st.write(cluster_df)


    def main(self):
        st.title('Your Words Laboratory')

        with st.expander('å½¢æ…‹ç´ è§£æãƒ•ã‚©ãƒ¼ãƒ ', expanded=True):
            with st.form(key='select_form'):
                options = st.multiselect('å“è©ã®é¸æŠ', self.all_parts, ['åè©'])
                st.caption('â€» å½¢çŠ¶è©ã¨ã¯ã€UniDicã®å“è©ä½“ç³»ã§ã€å½¢å®¹å‹•è©èªå¹¹ã‚’æ„å‘³ã—ã¦ã„ã¾ã™ã€‚')
                st.text('ã“ã‚Œã¾ã§ã®æ›¸ãè¾¼ã¿å†…å®¹ã‚’å½¢æ…‹ç´ è§£æã—ã€ä¸Šè¨˜ã®å“è©ã«è©²å½“ã™ã‚‹è¨€è‘‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚')
                st.write('')

                stop_line = st.text_input('ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å…¥åŠ›', placeholder='ã“ã“ã«èª­ç‚¹ã€Œã€ã€åŒºåˆ‡ã‚Šã§è¨€è‘‰ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãã‚Œã‚‰ã‚’é™¤å¤–ã—ã¦è§£æã—ã¾ã™ã€‚')
                stop_words = stop_line.split('ã€')
                stop_words = [self.cor.preprocessing(word) for word in stop_words]

                if self.authority:
                    input_users = st.multiselect('ãƒ¦ãƒ¼ã‚¶ãƒ¼åã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼', self.users) 
                else:
                    input_users = [self.user]
                    
                submit_btn = st.form_submit_button('å®Ÿè¡Œ')

        if submit_btn:
            if not options:
                st.error("æœ€ä½ã§ã‚‚ä¸€ã¤ä»¥ä¸Šã®å“è©ã‚’é¸æŠã—ã¦ä¸‹ã•ã„")
            
            else:
                try:
                    user_df = self.get_user_df(input_users)
                    target_corpus = user_df['words'].values.tolist()
                    self.cor.morphological_analysis(' '.join(target_corpus), options, stop_words)
                    
                    self.display_tabs()

                except ValueError:
                    st.warning('è§£æãŒå®Ÿè¡Œã•ã‚Œãªã„å ´åˆã¯ã€ã¾ã è©²å½“ã™ã‚‹è¨€è‘‰ãŒæ›¸ãè¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚')
                    st.info('''
                    ä¸»æˆåˆ†åˆ†æã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®çµæœã®ã¿ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€è©²å½“ã™ã‚‹è¨€è‘‰ã®ã†ã¡ã€
                    å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’å­¦ç¿’æ¸ˆã¿ã®ã‚‚ã®ãŒäºŒã¤æœªæº€ã®ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã„ã¾ã™ã€‚
                    ''')
            

cor = Corpus3(st.session_state)
page = Page3(st.session_state, cor)

page.check_login_status()
