import streamlit as st
import sqlite3
import time
import MeCab
from wordcloud import WordCloud
from collections import defaultdict
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import pickle
import gensim
import mojimoji
import unicodedata



fpath = './IPAfont00303/ipam.ttf'
all_parts = ['åè©','ä»£åè©','å½¢çŠ¶è©','é€£ä½“è©','å‰¯è©','æ¥ç¶šè©','æ„Ÿå‹•è©',
            'å‹•è©','å½¢å®¹è©','åŠ©å‹•è©','åŠ©è©','æ¥é ­è¾','æ¥å°¾è¾','è¨˜å·','è£œåŠ©è¨˜å·']

# å‰å‡¦ç†ï¼ˆå˜èªã®æ­£è¦åŒ–ï¼‰
def preprocessing(text):  
    result = mojimoji.zen_to_han(text, kana=False)
    result = mojimoji.han_to_zen(result, ascii=False)
    result = unicodedata.normalize('NFKC', result)
    return result


def morphological_analysis(text, selected_parts, stop_words):
    tagger = MeCab.Tagger()
    node = tagger.parseToNode(text)
    dic = defaultdict(int)
    morpheme_words = []

    while  node:
        surface = node.surface
        surface = preprocessing(surface)

        if node.feature.startswith('BOS/EOS') or surface in stop_words:
            pass
        else:
            f = node.feature.split(',')      
            pos1 = f[0]

            if pos1 in selected_parts and surface != '':
                dic[(surface, pos1)] += 1
                morpheme_words.append(surface)

        node = node.next
        
    sorted_items = sorted(dic.items(), key = lambda x: -x[1])
    merged_list = []

    for k, v in sorted_items:
        merged_list.append([k[0], k[1], v])

    return merged_list, morpheme_words, sorted_items


def show_results(user, options, stop_words):
    comment = st.empty()
    comment.text('Start!')
    latest_iteration = st.empty()
    bar = st.progress(0)

    conn = sqlite3.connect('database.db')
    c = conn.cursor()
    query = 'SELECT writer, words FROM diary ;'
    df = pd.read_sql(query, con = conn)
    c.close()

    try:
        user_df = df[df['writer']==user]
        corpus = user_df['words'].values.tolist()

        total_list, series, items = morphological_analysis(' '.join(corpus), options, stop_words)
        full_text = ' '.join(series)
        df = pd.DataFrame(total_list)
        df.columns = ['è¡¨å±¤å½¢','å“è©','åº¦æ•°']
        # df = df.sort_values('åº¦æ•°', ascending=False)
        keys = [_[0][0] for _ in items]
        values = [_[1] for _ in items[:10]]
        

        wc = WordCloud(background_color='white', width=960, height=640, font_path=fpath)
        wc.generate(full_text)
        wc.to_file('wc.png')

        for i in range(100):
            latest_iteration.text(f'{i+1}% Complete')
            bar.progress(i + 1)
            time.sleep(0.005)
        comment.text('Done!')
        '##'
        st.subheader('è§£æçµæœ')
        tab1, tab2, tab3, tab4 = st.tabs(['ğŸŒ© Word cloud', 'ğŸ“Š TOP10', 'ğŸ“‰ Vectors', 'ğŸ’¥ Clusters'])

        with tab1:
            img = Image.open('wc.png')
            st.image(img, caption='ã‚ãªãŸã®æ—¥è¨˜ã®å†…å®¹ã‹ã‚‰ä½œæˆã•ã‚ŒãŸãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰', use_column_width=True)

        with tab2:
            fig, ax = plt.subplots()
            ax.bar(keys[:10], values)
            fig.suptitle('å‡ºç¾é »åº¦ TOP10')
            st.pyplot(fig)

        st.sidebar.header('Below is a DataFrame:')
        st.sidebar.write('è©²å½“ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§', df)
        st.sidebar.caption('â€» åˆ—åã‚’ã‚¯ãƒªãƒƒã‚¯ã§ã‚½ãƒ¼ãƒˆå¯èƒ½')    


        with open('fastText.ja.300.vec.pkl',  mode = 'rb') as fp:
            model = pickle.load(fp)

        # vectors = [model[key] for key in keys]
        vectors = []
        keys_in_model = []

        for key in keys:
            if key in model.index_to_key:
                vectors.append(model[key])
                keys_in_model.append(key)

        model2d = PCA(n_components=2, whiten=True)
        model2d.fit(np.array(vectors[:30]).T)

        with tab3:
            fig, ax = plt.subplots()
            ax.scatter(model2d.components_[0],model2d.components_[1])
            for (x,y), name in zip(model2d.components_.T, keys_in_model[:30]):
                ax.annotate(name, (x,y))
            fig.suptitle('å˜èªãƒ™ã‚¯ãƒˆãƒ«ã®åˆ†å¸ƒï¼ˆä¸Šä½ï¼“ï¼å˜èªã¾ã§ï¼‰')
            st.pyplot(fig)
            st.caption('â€» å¯è¦–åŒ–ã®ãŸã‚ã€ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ã‚’300æ¬¡å…ƒã‹ã‚‰2æ¬¡å…ƒã«æ¬¡å…ƒå‰Šæ¸›ã—ã¦ã„ã¾ã™')  

        # n_clusters = len(keys_in_model) // 10 + 1
        n_clusters = int(np.sqrt(len(keys_in_model))) + 1
        kmeans_model = KMeans(n_clusters=n_clusters, verbose=1, random_state=0) 
        kmeans_model.fit(vectors)

        cluster_labels = kmeans_model.labels_
        cluster_to_words = defaultdict(list)
        for cluster_id, word in zip(cluster_labels, keys_in_model):
            cluster_to_words[cluster_id].append(word)

        with tab4:
            st.write('ä¼¼ã¦ã„ã‚‹è¨€è‘‰ã®ã‚°ãƒ«ãƒ¼ãƒ—')
            for i, words in enumerate(cluster_to_words.values()):
                cluster_df = pd.DataFrame(words).T
                cluster_df.index = [f'cluster {i}']
                # st.write(f'cluster{i}')
                st.write(cluster_df)
        #     st.table(words)
        # cluster_df = cluster_to_words.values()
        # st.dataframe(cluster_df)
                 

    except ValueError:
        st.warning('è§£æãŒå®Ÿè¡Œã•ã‚Œãªã„å ´åˆã¯ã€ã¾ã è©²å½“ã™ã‚‹è¨€è‘‰ãŒæ›¸ãè¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚')
        st.info('''
        å‡ºç¾é »åº¦ã®æ£’ã‚°ãƒ©ãƒ•ã¾ã§ã—ã‹è¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€è©²å½“ã™ã‚‹è¨€è‘‰ã®ã†ã¡ã€å˜èªãƒ™ã‚¯ãƒˆãƒ«ã‚’å­¦ç¿’
        æ¸ˆã¿ã®ã‚‚ã®ãŒä¸€ã¤ä»¥ä¸‹ã®ãŸã‚ã€ä¸»æˆåˆ†åˆ†æã‚„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã¦ã„ã¾ã›ã‚“ã€‚
        ''')
    


if 'user' not in st.session_state:
    st.info("ãƒ­ã‚°ã‚¤ãƒ³å¾Œã«åˆ©ç”¨å¯èƒ½ã§ã™")

else:
    user = st.session_state['user']

    st.title('Analysis of Your Words')
    st.write('å½¢æ…‹ç´ è§£æãƒ•ã‚©ãƒ¼ãƒ ')

    with st.form(key='select_form'):
        options = st.multiselect('å“è©ã®é¸æŠ', all_parts, ['åè©'])
        st.caption('â€» å½¢çŠ¶è©ã¨ã¯ã€UniDicã®å“è©ä½“ç³»ã§ã€å½¢å®¹å‹•è©èªå¹¹ã‚’æ„å‘³ã—ã¦ã„ã¾ã™ã€‚')
        st.text('ã“ã‚Œã¾ã§ã®æ›¸ãè¾¼ã¿å†…å®¹ã‚’å½¢æ…‹ç´ è§£æã—ã€ä¸Šè¨˜ã®å“è©ã«è©²å½“ã™ã‚‹è¨€è‘‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚')
        st.write('')

        stop_line = st.text_area('ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å…¥åŠ›', placeholder='ã“ã“ã«è¨€è‘‰ã‚’èª­ç‚¹ã€Œã€ã€åŒºåˆ‡ã‚Šã§ã„ãã¤ã‹å…¥åŠ›ã™ã‚‹ã¨ã€ãã‚Œã‚‰ã‚’é™¤å¤–ã—ã¦è§£æã—ã¾ã™ã€‚')
        stop_words = stop_line.split('ã€')
        stop_words = [preprocessing(word) for word in stop_words]
        submit_btn = st.form_submit_button('å®Ÿè¡Œ')

    if submit_btn:
        if not options:
            st.error("æœ€ä½ã§ã‚‚ä¸€ã¤ä»¥ä¸Šã®å“è©ã‚’é¸æŠã—ã¦ä¸‹ã•ã„")
        else:
            show_results(user, options, stop_words)
